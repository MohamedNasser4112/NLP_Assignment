{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Nasser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Nasser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Nasser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download NLTK resources\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text from HTML\n",
    "def extract_text_from_html(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extract text from paragraphs (you can modify this based on your requirements)\n",
    "        paragraphs = soup.find_all('p')\n",
    "        \n",
    "        # Combine paragraphs into a single string\n",
    "        text = ' '.join([p.get_text() for p in paragraphs])\n",
    "        \n",
    "        return text\n",
    "    else:\n",
    "        print(\"Failed to retrieve HTML content\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words:\n",
      "{'student', 'cognition', 'technology', 'root', 'solving', 'drawback', 'dependency', 'nevertheless', 'involve', 'emulate', 'already', 'property', 'hard', 'future', 'still', 'major', 'earliest', 'framework', 'corpus', 'criterion', 'series', 'pursued', 'information', 'replaced', 'natural', 'transformational', 'patient', 'contextual', 'see', 'premise', 'producing', 'cpu', 'advance', 'broadly', 'coauthor', 'complex', 'within', 'handcoding', 'partofspeech', 'writing', 'branch', 'principle', 'healthcare', 'operationalizable', 'modelling', 'modeling', 'three', 'karl', 'medicine', 'length', 'applied', 'probabilistic', 'behaviour', 'support', 'year', 'mainstream', 'process', 'datasets', 'likewise', 'called', 'others', 'networkstyle', 'translation', 'several', 'aspect', 'eg', 'british', 'technical', 'explicit', 'revolution', 'human', 'rarely', 'representation', 'machinery', 'study', 'many', 'tom', 'understanding', 'recognition', 'question', 'confronts', 'intelligence', 'example', 'giving', 'mental', 'well', 'dominance', 'general', 'increase', 'dictionary', 'automated', 'application', 'especially', 'following', 'problem', 'privacy', 'however', 'computer', 'insight', 'showing', 'manipulating', 'approach', 'development', 'subfield', 'recently', 'wordvec', 'machinelearning', 'thennewlyinvented', 'explainability', 'interdisciplinary', 'hand', 'anymore', 'machine', 'engineering', 'acl', 'involves', 'various', 'coarse', 'period', 'symbol', 'help', 'notion', 'feature', 'functional', 'subdivided', 'john', 'artificial', 'popularity', 'include', 'set', 'use', 'embeddings', 'caused', 'apparent', 'multilayer', 'word', 'needed', 'chomskyan', 'trend', 'necessary', 'age', 'actr', 'matching', 'devising', 'room', 'require', 'care', 'cluster', 'part', 'trajectory', 'sequencetosequence', 'titled', 'concerned', 'subtasks', 'became', 'would', 'note', 'represents', 'mikolov', 'strong', 'goal', 'single', 'recurrent', 'llm', 'university', 'ie', 'nuance', 'particular', 'trained', 'accurately', 'computational', 'inaccessible', 'went', 'stateoftheart', 'cognitive', 'categorize', 'closely', 'along', 'inefficiency', 'finding', 'psycholinguistics', 'convenience', 'improve', 'methodology', 'thought', 'ai', 'maintained', 'due', 'system', 'bengio', 'collection', 'nlp', 'lessening', 'million', 'announced', 'moore', 'london', 'record', 'category', 'developmental', 'presence', 'organize', 'knowledge', 'text', 'idea', 'network', 'conference', 'test', 'symbolic', 'phrasebook', 'discouraged', 'based', 'combining', 'analyze', 'document', 'speech', 'serve', 'larger', 'intelligent', 'achieve', 'free', 'division', 'ended', 'heuristic', 'gradual', 'elaborate', 'language', 'experiment', 'protect', 'time', 'yoshua', 'neuroscientist', 'tree', 'alan', 'theoretician', 'given', 'computing', 'method', 'searles', 'phd', 'tie', 'electronic', 'includes', 'theory', 'realworld', 'scientific', 'j', 'george', 'speaking', 'historical', 'made', 'psychology', 'two', 'although', 'either', 'specifically', 'deep', 'late', 'shared', 'applying', 'limited', 'mostly', 'coupled', 'among', 'large', 'neural', 'though', 'articulated', 'mind', 'primarily', 'tagging', 'lakoff', 'perspective', 'data', 'context', 'brno', 'historically', 'ngram', 'action', 'intertwined', 'list', 'separate', 'networkbased', 'one', 'rule', 'extract', 'heritage', 'direction', 'introduction', 'technique', 'capture', 'content', 'aid', 'semantic', 'technically', 'comprehension', 'task', 'power', 'towards', 'turing', 'conll', 'step', 'decision', 'science', 'otherwise', 'higherlevel', 'friston', 'contained', 'featuring', 'turn', 'acquiring', 'intermediate', 'processing', 'ability', 'algorithm', 'frequently', 'perceptron', 'alignment', 'obsolete', 'using', 'multimodal', 'challenge', 'possible', 'build', 'rulebased', 'old', 'offer', 'measured', 'develop', 'stemming', 'underpinnings', 'observed', 'grammar', 'underlies', 'advanced', 'law', 'transformation', 'direct', 'winter', 'neuroscience', 'researched', 'wellsummarized', 'proposed', 'used', 'similar', 'inherent', 'theoretical', 'interpretation', 'tool', 'published', 'operationalization', 'new', 'statistical', 'answer', 'increasingly', 'topic', 'naturallanguage', 'steady', 'learning', 'best', 'whose', 'experience', 'sens', 'layer', 'linguistics', 'parsing', 'important', 'markov', 'starting', 'partly', 'defining', 'generation', 'result', 'commonly', 'le', 'previously', 'including', 'sort', 'manipulate', 'seeking', 'addressed', 'health', 'energy', 'revived', 'capable', 'field', 'article', 'model', 'advantage', 'flurry', 'mids', 'emulates', 'widespread', 'longstanding', 'area', 'research', 'ifthen', 'construction', 'since', 'chinese', 'college', 'extrapolate', 'simple', 'become', 'end', 'refers', 'hidden', 'lookup', 'first', 'handwritten', 'overperformed', 'uptake'}\n"
     ]
    }
   ],
   "source": [
    "# URL of the webpage you want to extract text from\n",
    "url = 'https://en.wikipedia.org/wiki/Natural_language_processing'\n",
    "\n",
    "# Extract text from HTML\n",
    "html_text = extract_text_from_html(url)\n",
    "\n",
    "if html_text:\n",
    "    # Data cleaning: Remove symbols and special characters\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', html_text)\n",
    "    \n",
    "    # Normalization: Convert text to lowercase\n",
    "    normalized_text = cleaned_text.lower()\n",
    "    \n",
    "    # Tokenization: Split text into words\n",
    "    words = word_tokenize(normalized_text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Initialize lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "    \n",
    "    # Get unique words\n",
    "    unique_words = set(lemmatized_words)\n",
    "    \n",
    "    # Print unique words\n",
    "    print(\"Unique words:\")\n",
    "    print(unique_words)\n",
    "else:\n",
    "    print(\"Error extracting text from HTML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'j', 'ie', 'ai', 'le', 'eg'}\n"
     ]
    }
   ],
   "source": [
    "Less_than_3ch = {word for word in unique_words if(len(word)< 3)}\n",
    "print(Less_than_3ch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
